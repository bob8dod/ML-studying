{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cliff Walking Example",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bob8dod/ML-studying/blob/main/RL/Cliff_Walking_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlwSKTZhgDTM"
      },
      "source": [
        "import gym\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv # Cliff Walking 환경"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4XRfMCWq9GL"
      },
      "source": [
        "QtoPolicy Class는 학습된 Q-value를 입력하면 해당하는 Q-value의 greedy 정책이 출력되도록 하는 함수 `printPolicy`를 구성하는 Class입니다. Q-learning 및 Sarsa를 이용하여 학습된 정책을 출력하기 위해 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLfl84A7nfo7"
      },
      "source": [
        "class QtoPolicy:\n",
        "    def __init__(self):\n",
        "        self.action = ['↑','→','↓','←']\n",
        "    \n",
        "    def printPolicy(self, Q):\n",
        "        policy = np.array([np.argmax(Q[key]) if key in Q else -1 for key in np.arange(48)])\n",
        "        v = ([np.max(Q[key]) if key in Q else 0 for key in np.arange(48)])\n",
        "        actions = np.stack([self.action for _ in range(len(policy))], axis=0)\n",
        "        \n",
        "        print(np.take(actions, np.reshape(policy, (4, 12))))\n",
        "        print('')"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "761Rn8P2rPR7"
      },
      "source": [
        "아래는 Q-learning 알고리즘을 수행하는 Class의 정의입니다.\n",
        "*   `update()` 메쏘드의 경우 state, action, reward, next_state, next_action이 주어졌을 때 Q-value를 업데이트하는 함수입니다.\n",
        "*   `act()` 메쏘드의 경우 $\\epsilon$-greedy 정책에 따라 action을 선택하는 함수입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aod3D2Lnkw7_"
      },
      "source": [
        "class QLearning:\n",
        "    def __init__(self):\n",
        "        self.action_no = 4 # state에서 취할 수 있는 action의 개수\n",
        "        self.alpha = 0.01 # 반영률\n",
        "        self.gamma = 0.9 # 미래의 값에 두는 가중치. 0.9로 미래의 값에 중요도를 높임\n",
        "        self.epsilon = 0.2 # 입실론, 임의의 action을 취하는 확률\n",
        "        self.q_values = defaultdict(lambda: [0.0] * self.action_no) # Q-Table\n",
        "        \n",
        "    # 학습과정\n",
        "    def update(self, state, action, reward, next_state, next_action): \n",
        "        q_value = self.q_values[state][action] # 현 상태에서 주어진 action을 취했을 때의 q_value\n",
        "        next_q_value = max(self.q_values[next_state]) # 다음 상태에서의 q_value\n",
        "        # Q-learning 같은 경우에서의 Target정책은 무조건 탐욕적으로 행동을 결정하기에 \n",
        "        # max값을 통해 가장 큰 q_value를 얻는 action을 취했을 때의 q_value값을 next_q_value로 저장\n",
        "        \n",
        "        td_error = reward + self.gamma * next_q_value - q_value \n",
        "        # 내가 따르고자 하는 Target정책 과 실제 행동이 이루어지는 정책간의  q_value 차이\n",
        "        self.q_values[state][action] = q_value + self.alpha * td_error \n",
        "        # td_error를 반영하여 Q(S,A)를 업데이트\n",
        "    \n",
        "    # 입실론 그리디 정책 (행동을 결정)\n",
        "    def act(self, state):\n",
        "        if np.random.rand() < self.epsilon: # 입실론보다 작은 확률로\n",
        "            action = np.random.choice(self.action_no) # 행동을 임의로 선택\n",
        "        else: # '1-입실론' 확률로 \n",
        "            q_values = self.q_values[state] # 현재 state가 가질 수 있는 모든 Q값\n",
        "            action = np.argmax(q_values) \n",
        "            # 탐욕적인 행동 선택 진행 (해당 Q값 들 중 가장 큰 값의 Q값을 가지는 action 선택)\n",
        "        return action # action 반환"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Jq-C_Vr1HV"
      },
      "source": [
        "아래는 Sarsa 알고리즘을 수행하는 Class의 정의입니다.\n",
        "*   `update()` 메쏘드의 경우 state, action, reward, next_state, next_action이 주어졌을 때 Q-value를 업데이트하는 함수입니다.\n",
        "*   `act()` 메쏘드의 경우 $\\epsilon$-greedy 정책에 따라 action을 선택하는 함수입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moiYtfA0pYiY"
      },
      "source": [
        "class Sarsa:\n",
        "    def __init__(self):\n",
        "        self.action_no = 4 # state에서 취할 수 있는 action의 개수\n",
        "        self.alpha = 0.01 # 반영률\n",
        "        self.gamma = 0.9 # 미래의 값에 두는 가중치. 0.9로 미래의 값에 중요도를 높임\n",
        "        self.epsilon = 0.2  # 입실론, 임의의 action을 취하는 확률\n",
        "        self.q_values = defaultdict(lambda: [0.0] * self.action_no) # Q-Table\n",
        "\n",
        "    # 학습과정\n",
        "    def update(self, state, action, reward, next_state, next_action):\n",
        "        q_value = self.q_values[state][action] # 현 상태에서 주어진 action을 취했을 때의 q_value\n",
        "        next_q_value =self.q_values[next_state][next_action] # 다음 상태에서의 q_value\n",
        "        # Q-learning과의 차이점.Target 정책을 사용하지 않고 behavior 정책을 그대로 사용하여 학습 진행\n",
        "        # 그렇기에 behavior 정책을 통해 다음 state의 다음 action을 받아와 next_q_value 측정\n",
        "\n",
        "        td_error = reward + self.gamma * next_q_value - q_value # 경험을 통한 q_value 값의 차이 측정\n",
        "        self.q_values[state][action] = q_value + self.alpha * td_error \n",
        "        # td_error를 반영하여 Q(S,A)를 업데이트 (실제적인 경험을 통해 가치함수를 업데이트하여 개선)\n",
        "    \n",
        "    # 입실론 그리디 정책 (행동을 결정)\n",
        "    def act(self, state):\n",
        "        if np.random.rand() < self.epsilon: # 입실론보다 작은 확률로\n",
        "            action = np.random.choice(self.action_no) # 행동을 임의로 선택\n",
        "        else: # '1-입실론' 확률로 \n",
        "            q_values = self.q_values[state] # 현재 state가 가질 수 있는 모든 Q값\n",
        "            action = np.argmax(q_values) \n",
        "            # 탐욕적인 행동 선택 진행 (해당 Q값 들 중 가장 큰 값의 Q값을 가지는 action 선택)\n",
        "        return action # action 반환"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75i5h8kmr9Mp"
      },
      "source": [
        "OpenAI Gym에서의 Cliff Walking 환경을 로드하고 해당하는 환경을 살펴보기 위해 `render()` 메쏘드를 사용해봅니다.\n",
        "그리고 `env.nS` 및 `env.nA` 변수를 통해 해당 환경의 state 및 action 개수를 확인합니다.\n",
        "\n",
        "Cliff Walking 환경에서 각 state는 grid에서의 위치, action은 'up', 'right', 'down', 'left' 방향을 의미합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orqk6rcLg-uZ",
        "outputId": "2e6090af-f4e7-47bb-b483-f4e3c86aae79"
      },
      "source": [
        "env = CliffWalkingEnv()\n",
        "env.render()\n",
        "print ('Number of states: ', env.nS)\n",
        "print ('Number of actions :', env.nA)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Number of states:  48\n",
            "Number of actions : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og4Di2fGll2B"
      },
      "source": [
        "주어진 Q-value에서 greedy policy를 출력하는 QtoPolicy Class를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3H1tlkwnSVu"
      },
      "source": [
        "policy = QtoPolicy()"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unvFLwuGsaEQ"
      },
      "source": [
        "Q-learning Class를 정의하고 5000 episode 동안 학습을 수행합니다.\n",
        "\n",
        "Gym 라이브러리의 환경에서는 `step(action)` 메쏘드를 통해 해당하는 time-step에서 action을 수행한 효과를 얻을 수 있습니다. 해당 메쏘드에서는 action을 수행하여 얻어지는 보상 (reward), 다음 상태 (next_state), done (episode 종료여부) 등이 출력으로 주어집니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "patW0RK5olea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38aa1fe-ffaa-4b3b-9325-d4aa06f9c83e"
      },
      "source": [
        "agent_QL = QLearning() # Q-learning instance 생성\n",
        "for ep in range(1,5001): # 5000 episode 동안 학습 수행\n",
        "    done = False\n",
        "    state = env.reset() # 초기 state\n",
        "    action = agent_QL.act(state) # 초기 state에서의 action\n",
        "    \n",
        "    ep_rewards = 0 # 누적 보상\n",
        "    while not done: # 종료될때까지 반복\n",
        "        next_state, reward, done, info = env.step(action) # action을 통한 다음 state, reward 종료여부 반환\n",
        "\n",
        "        next_action = agent_QL.act(next_state) # 다음 state에서의 다음 action (A_t+1)\n",
        "\n",
        "        agent_QL.update(state, action, reward, next_state, next_action) # 학습 진행\n",
        "        \n",
        "        ep_rewards += reward # 누적 보상에 현 보상 추가\n",
        "        state = next_state # state 업데이트\n",
        "        action = next_action # action 업데이트\n",
        "    if ep % 500 == 0: #500 episode 마다 출력\n",
        "        print(\"episode: {}, rewards: {}\".format(ep, ep_rewards))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 500, rewards: -175\n",
            "episode: 1000, rewards: -31\n",
            "episode: 1500, rewards: -17\n",
            "episode: 2000, rewards: -13\n",
            "episode: 2500, rewards: -120\n",
            "episode: 3000, rewards: -17\n",
            "episode: 3500, rewards: -445\n",
            "episode: 4000, rewards: -13\n",
            "episode: 4500, rewards: -124\n",
            "episode: 5000, rewards: -21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGnPP-qAsoAY"
      },
      "source": [
        "Sarsa에 대해서도 같은 방식으로 학습을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv2LB-xEqYL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4e02aa-be8d-489a-ae53-bd9ef5288e42"
      },
      "source": [
        "agent_Sa = Sarsa() # Q-learning instance 생성\n",
        "for ep in range(1,5001): # 5000 episode 동안 학습 수행\n",
        "    done = False\n",
        "    state = env.reset() # 초기 state\n",
        "    action = agent_Sa.act(state) # 초기 state에서의 action\n",
        "    \n",
        "    ep_rewards = 0 # 누적 보상\n",
        "    while not done: # 종료될때까지 반복\n",
        "        next_state, reward, done, info = env.step(action)  # action을 통한 다음 state, reward 종료여부 반환\n",
        "\n",
        "        next_action = agent_Sa.act(next_state) # 다음 state에서의 다음 action (A_t+1)\n",
        "\n",
        "        agent_Sa.update(state, action, reward, next_state, next_action) # 학습 진행\n",
        "        \n",
        "        ep_rewards += reward # 누적 보상에 현 보상 추가\n",
        "        state = next_state # state 업데이트\n",
        "        action = next_action # action 업데이트\n",
        "    if ep % 500 == 0: #500 episode 마다 출력\n",
        "        print(\"episode: {}, rewards: {}\".format(ep, ep_rewards))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 500, rewards: -42\n",
            "episode: 1000, rewards: -28\n",
            "episode: 1500, rewards: -21\n",
            "episode: 2000, rewards: -30\n",
            "episode: 2500, rewards: -17\n",
            "episode: 3000, rewards: -17\n",
            "episode: 3500, rewards: -22\n",
            "episode: 4000, rewards: -20\n",
            "episode: 4500, rewards: -19\n",
            "episode: 5000, rewards: -19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox-__wN3s3LR"
      },
      "source": [
        "학습된 Q-value를 이용하여 학습된 정책을 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqHh3TRcs3dh",
        "outputId": "8f209590-4ebb-4de0-de83-db22e61f17e2"
      },
      "source": [
        "print('Learned policy by Q-learning')\n",
        "policy.printPolicy(agent_QL.q_values)\n",
        "print('Learned policy by Sarsa')\n",
        "policy.printPolicy(agent_Sa.q_values)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned policy by Q-learning\n",
            "[['→' '→' '→' '→' '→' '→' '→' '↓' '→' '→' '↓' '↓']\n",
            " ['↑' '↑' '↑' '→' '↑' '↓' '↓' '→' '↓' '↓' '↓' '↓']\n",
            " ['→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '←' '←' '←' '←' '←' '←' '←' '←' '←' '←' '↑']]\n",
            "\n",
            "Learned policy by Sarsa\n",
            "[['→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '←' '↑' '→' '→' '←' '↑' '→' '→' '←' '→' '↓']\n",
            " ['↑' '←' '←' '←' '←' '←' '←' '←' '←' '←' '←' '↑']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NFlcKhvoq4u"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": 86,
      "outputs": []
    }
  ]
}