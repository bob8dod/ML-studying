{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20_DL_Model_(GAN_generator,discriminator).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFYs9y7Cv86dwnoq/w94Ly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bob8dod/ML-studying/blob/main/ML_DL/20_DL_Model_(GAN_generator%2Cdiscriminator).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tqou6fpea2u"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential, Model #Model_make new Model\n",
        "from tensorflow.keras.layers import Input, Dense,Flatten,Dropout,Activation, LeakyReLU\n",
        "from tensorflow.keras.layers import Reshape, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA7qeDCkyu4A"
      },
      "source": [
        "seed=0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jRcRUexoxtK"
      },
      "source": [
        "#Generator Model _ Make fake image\n",
        "generator = Sequential()\n",
        "generator.add(Dense(128*7*7,input_dim=100,activation=LeakyReLU(0.2))) #7x7x128\n",
        "generator.add(BatchNormalization()) # for normalization value\n",
        "generator.add(Reshape((7,7,128))) #7*7*128 -> 7x7,128 / reshape the data\n",
        "generator.add(UpSampling2D()) # make 2mulitply size -> 14x14, 128\n",
        "generator.add(Conv2D(64,kernel_size=5, padding='same')) #14x14, 64\n",
        "generator.add(BatchNormalization()) # for normalization value\n",
        "generator.add(Activation(LeakyReLU(0.2)))\n",
        "generator.add(UpSampling2D()) # make 2mulitply size -> 28X28, 64\n",
        "generator.add(Conv2D(1,kernel_size=5, padding='same',activation='tanh')) #result of fake image -> 28x28, 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9pLF0_Dq2Zw"
      },
      "source": [
        "#Discriminator Model _ make a distinction of fake image\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Conv2D(64,kernel_size=5, strides=2,input_shape=(28,28,1),padding='same')) # 13x13, 64\n",
        "discriminator.add(Activation(LeakyReLU(0.2)))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Conv2D(128,kernel_size=5,strides=2,padding='same')) #5x5, 128\n",
        "discriminator.add(Activation(LeakyReLU(0.2)))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Flatten()) #5x5, 128 -> 5*5*128\n",
        "discriminator.add(Dense(1,activation='sigmoid')) # result of distinction\n",
        "discriminator.compile(loss='binary_crossentropy',optimizer='adam')\n",
        "discriminator.trainable = False #For not saving the change in weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiawayIqtiFr",
        "outputId": "5cf7dd0c-dcfc-49ed-c8dc-1357db072f29"
      },
      "source": [
        "#GAN _ generator + discriminator\n",
        "ginput = Input(shape=(100,)) #Make input _ vector, size of 100\n",
        "dis_output = discriminator(generator(ginput)) #set output\n",
        "gan = Model(ginput, dis_output) # Make Model _ connect generator and discriminator\n",
        "gan.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "gan.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "sequential_5 (Sequential)    (None, 28, 28, 1)         865281    \n",
            "_________________________________________________________________\n",
            "sequential_6 (Sequential)    (None, 1)                 212865    \n",
            "=================================================================\n",
            "Total params: 1,078,146\n",
            "Trainable params: 852,609\n",
            "Non-trainable params: 225,537\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz43em2WuYGS",
        "outputId": "17ab284b-b79d-4a2c-caf9-95b021e1e3bf"
      },
      "source": [
        "#Train GAN\n",
        "def gan_train(epochs,batch_size,savingpoint):\n",
        "  (X_train, _), (_, _) = mnist.load_data()  # We use only the X_train Images\n",
        "  X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') # Reshape!\n",
        "  X_train = (X_train - 127.5) / 127.5  #set image's pixel, -1 ~ +1\n",
        "\n",
        "  true = np.ones((batch_size, 1)) #True label\n",
        "  fake = np.zeros((batch_size, 1)) #Fake label\n",
        "\n",
        "  for i in range(epochs + 1): #Train\n",
        "    #train discriminator through real images\n",
        "    idx = np.random.randint(0, X_train.shape[0],batch_size) # set random index\n",
        "    imgs = X_train[idx] #real images\n",
        "    d_loss_real = discriminator.train_on_batch(imgs,true) #Train! to 1\n",
        "\n",
        "    #train discriminator through fake images\n",
        "    noise = np.random.normal(0,1,(batch_size,100)) #set noise with random value, row:batch_size, column: 100_input size\n",
        "    gen_imgs = generator.predict(noise) #fake images\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs,fake) #Train! to 0\n",
        "\n",
        "    d_loss = np.add(d_loss_real,d_loss_fake)/2 #discriminator loss\n",
        "    g_loss = gan.train_on_batch(noise,true) #GAN loss\n",
        "    if i%200==0:\n",
        "            print('epoch:%d' % i, ' d_loss:%.4f' % d_loss, ' g_loss:', g_loss)\n",
        "\n",
        "gan_train(2000, 32, 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0  d_loss:0.7239  g_loss: [1.148545265197754, 0.34375]\n",
            "epoch:200  d_loss:0.3169  g_loss: [2.0236964225769043, 0.125]\n",
            "epoch:400  d_loss:0.2287  g_loss: [3.0745723247528076, 0.03125]\n",
            "epoch:600  d_loss:0.5653  g_loss: [1.276777744293213, 0.375]\n",
            "epoch:800  d_loss:0.3586  g_loss: [2.1179730892181396, 0.0625]\n",
            "epoch:1000  d_loss:0.4773  g_loss: [1.999975323677063, 0.25]\n",
            "epoch:1200  d_loss:0.3201  g_loss: [1.8208882808685303, 0.1875]\n",
            "epoch:1400  d_loss:0.4739  g_loss: [1.8242599964141846, 0.09375]\n",
            "epoch:1600  d_loss:0.4960  g_loss: [1.7764948606491089, 0.15625]\n",
            "epoch:1800  d_loss:0.6077  g_loss: [1.6049537658691406, 0.15625]\n",
            "epoch:2000  d_loss:0.4040  g_loss: [1.6755828857421875, 0.15625]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}