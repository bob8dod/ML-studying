{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17_DL_Model_(NLP_Tokenizer, pad_sequences, Embedding).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWwWMAS6oQaVAk+KWdrqoG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bob8dod/ML-studying/blob/main/ML_DL/17_DL_Model_(NLP_Tokenizer%2C_pad_sequences%2C_Embedding).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8yZPzoRgMkz"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences #for text padding(0)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense # NN: Flatten, Dense\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQjoatYRilPo"
      },
      "source": [
        "#set seed\n",
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG9bzrKejAXQ"
      },
      "source": [
        "#Setting Data\n",
        "docs = [\"Its funny\",\"great\",\"well made movie\",\n",
        "        \"want to recommand\",\"wish see again\",\n",
        "        \"dont know\",\"not good\",\"boring\",\"worst of ever seen\",\"shit\" ]\n",
        "label = np.array([1,1,1,1,1,0,0,0,0,0]) #for label, Y_train,,,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeh_4INajq6p",
        "outputId": "da78a747-5610-49ab-fd45-5c23c93bd63e"
      },
      "source": [
        "#Data preprocessing\n",
        "token = Tokenizer() #activate function\n",
        "token.fit_on_texts(docs) #apply on docs\n",
        "print(token.word_index) #check word_index\n",
        "\n",
        "x=token.texts_to_sequences(docs) #Create a new array filled with only the index of the token\n",
        "print(x)\n",
        "\n",
        "x_padded = pad_sequences(x,4) #padding the text length for 4 _ input_length\n",
        "print(x_padded)\n",
        "\n",
        "word_size = len(token.word_index) + 1 #add 1 for 0 index(hidden)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'its': 1, 'funny': 2, 'great': 3, 'well': 4, 'made': 5, 'movie': 6, 'want': 7, 'to': 8, 'recommand': 9, 'wish': 10, 'see': 11, 'again': 12, 'dont': 13, 'know': 14, 'not': 15, 'good': 16, 'boring': 17, 'worst': 18, 'of': 19, 'ever': 20, 'seen': 21, 'shit': 22}\n",
            "[[1, 2], [3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14], [15, 16], [17], [18, 19, 20, 21], [22]]\n",
            "[[ 0  0  1  2]\n",
            " [ 0  0  0  3]\n",
            " [ 0  4  5  6]\n",
            " [ 0  7  8  9]\n",
            " [ 0 10 11 12]\n",
            " [ 0  0 13 14]\n",
            " [ 0  0 15 16]\n",
            " [ 0  0  0 17]\n",
            " [18 19 20 21]\n",
            " [ 0  0  0 22]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81CmFix_khJp"
      },
      "source": [
        "#Model Setting\n",
        "model = Sequential()\n",
        "model.add(Embedding(word_size,8,input_length=4)) #Embedding(input_dim, output_dim, input_length)\n",
        "model.add(Flatten()) #Make 2D to 1D for NN\n",
        "model.add(Dense(1,activation='sigmoid')) #output layer\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #compiling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40NHYhW0nivi",
        "outputId": "c11a9f17-ba63-47eb-fd7d-269d3c5504e4"
      },
      "source": [
        "#Model opperating\n",
        "model.fit(x_padded,label,epochs=20) # fit(X,Y,epochs)\n",
        "print('\\nAccuracy: %.4f' %(model.evaluate(x_padded, label)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.6893 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.6000\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6845 - accuracy: 0.6000\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6821 - accuracy: 0.7000\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6797 - accuracy: 0.7000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6773 - accuracy: 0.7000\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.8000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6725 - accuracy: 0.8000\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6701 - accuracy: 0.8000\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6678 - accuracy: 0.8000\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6654 - accuracy: 0.9000\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6630 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6606 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6582 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6558 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6534 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6510 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6486 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6462 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6437 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.6413 - accuracy: 1.0000\n",
            "\n",
            "Accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}